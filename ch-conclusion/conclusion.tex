\newpage
\chapter{Conclusion}
\label{ch:conclusion}

In this thesis, we went through all the improvements we made for the GLAD-M25.
To overcome the I/O bottleneck, we introduced the ADIOS and ASDF, for storing 
model data and seismic data. To make the computation more efficient and preferment,
we rebuilt our tools using python and added new features into those software.
Workflow management tools help us to automate the inversion, reducing possible
human-introduced errors. The new weighting schema we proposed help to
mitigate the uneven distribution of seismic source and stations, especially
for dense regional seismic network such as US Arrays. Our new model is
generated using all the improvements we mentioned using 1,480 earthquakes and
10 conjugate gradient and L-BFGS iterations.

We presented our model together with many global and regional models. Through
those comparisons, we demonstrated that our model shows superior resolution
not only for upper mantle structures, but also shows clear images at depth.

\section{Future Work}

There are definitely lots of works could be done in the future. The lower hanging
fruit would be a deeper investigation in our model structures.
There are definitely lots of interesting geological structures emerging in the
GLAD-M25 model that should be investigated. Vp/Vs ratio on plumes is one of
the topics. We also see interesting subduction structures at South America,
Farallon slabs and Tibet. Our new model could provide us new insights into the
dynamics of the Earth.

There are also a few directions we would like to go for the third-gen of GLAD
models. First, there are still more earthquakes available. Our current earthquake
dataset covers most of the deep events from 1995 -- 2016 in the Global CMT Project. However,
there are still thousands of inter-mediate and shallow earthquakes no being
assimilated. We haven't added any events from 2017 and 2018 yet. We are planning
to push our dataset to 4,000 earthquakes in the near future. Our efforts 
in data format, software and tools would enable us to handle a even larger dataset
without any issue.

We could also add more parameters into our model inversion. Anelastic inversion
could be one of them. Anelastic attenuation is an important factor that affects
the amplitude of waveforms. From our amplitude histograms, it is obvious that 
there are quite a lot of space for the attenuation to be improved. Anisotropy
is another important factor for wave propagation. It will also provide new insights
into those highly heterogeneous regions, such as plumes and subduction regions.

We also want to push the model resolution from 17sec to 9sec. That will requires us
to have better source modelings. At global scale,
to ensure good signal-to-noise ratio, we usually picked events whose moment
magnitudes are between 5.5 to 7.2, and the duration of source would reaches ~17secs.
However, when the simulation reaches such short period, source time function
would really critical for the waveform fitting. Thus, to really use all the events,
we need to have a better estimation of the source time function, rather than using
simple Gaussian type function.

Source encoding could potentially bring new revolution into our inversion workflow. With such
technique, kernels could be calculated with one run(no gaps between forward and adjoint
simulation), so there is no need saved wavefield files, which still stressed the file system
a lot and remains our major challenges on HPC system.
challenges we faced now.

\section{Introduction}\label{sec:intro}

Striving to comprehend Earth's interior has been a longstanding pursuit for
humankind, and has been fantasized by many, from Dante Alighieri~\cite{Dante1320}
to Jules Vernes~\cite{Vernes1864}.
Seismologists see Earth's interior through seismic waves generated by seismic
sources such as earthquakes, oceanic noise or man-made explosions, and
recorded by seismic instruments deployed at the surface.
The information inherent to these seismic waves, which are sensitive
to physical parameters of the medium they propagate through, is used to construct
3D images of the Earth based on \emph{seismic tomography}.
%similar to the cat-scan in medicine.
Advances in the theory of wave propagation and 3D numerical solvers, supported by
dramatic increases in the amount and quality of seismic data as well as
the unprecedented amount of computational power provided by large-scale high-performance
computing centers, enables us to greatly improve our understanding of the
physics and chemistry of Earth's interior.

Adjoint methods are very efficient at 
incorporating 3D numerical wave simulations in seismic tomography.
They have, for instance, successfully been
applied to regional- and global-scale earthquake tomography
~\cite{tape2009adjoint, Fichtner09,
zhu2012structure, bozdaug2016global} and ---to some extent--- in exploration seismology studies
~\cite{Zhu2009, Luo2013}.

Adjoint tomography workflows consist of a series of iterations. Each iteration is
composed of a few shared operations (e.g., mesh generation, model updates) and
of a large number of embarrassingly parallel operations (e.g., forward and
adjoint simulations for each seismic event, pre- and post-processing of seismic
data).
One of the main computational challenges is to increase the quality of seismic
models while keeping the time to solution as short as possible. Having fast and
efficient solvers still remains an important concern~\cite{Rietmann2012}, but
new obstacles have emerged: large-scale experiments and big datasets create
bottlenecks in workflows causing significant I/O problems on HPC systems.

In this chapter, we devise and investigate strategies to scale global
adjoint tomography to unprecedented levels by assimilating data from thousands
of earthquakes. We elaborate on improvements targeting not only current
supercomputers, but also next generation systems, such as OLCF's \emph{Summit}. The
following remarks and developments stem from lessons learned while performing
simulations on OLCF's \emph{Titan} for the first-generation global adjoint tomography 
model, which is the result of 15 iterations and a limited dataset of 253 earthquakes
~\cite{bozdaug2016global}.

We begin in Section~\ref{sec:scientific_method} by laying down the scientific
methodology of the global adjoint tomography problem and providing explanations
of the scientific workflow and its components. We then follow a reductionist
approach, considering each component individually. Section~\ref{sec:solver}
examines the computational aspects of \texttt{SPECFEM3D\_GLOBE}
~\cite{KoTr02a, KoTr02b}, our 3D seismic wave
equation solver and the most computationally demanding part of our workflow. We
provide a brief overview and discuss the programming approach it follows. We also
present scalability analyses. Section~\ref{sec:computational_data} is 
closely related to the solver and describes the approach we chose to optimize
I/O for computational data, that is, data related to meshes and models. We then
describe a modern seismic data format in Section~\ref{sec:asdf}.
Assimilating a large number of seismic time series from numerous earthquakes requires
improvement of legacy seismic data formats, including provenance  information
and the ability to seamlessly integrate in a complex data processing chain.
With the previous points in mind, Section~\ref{sec:workflow_management}
returns to our initial holistic approach and discusses how to bring the global
adjoint tomography workflow bits and pieces under the control of a scientific
workflow management system. Finally, Section~\ref{sec:software_practices}
explains our approach to software practices, an often overlooked but crucial
part of scientific software development.

% Legacy seismic data formats were initially designed for specific seismic
% applications involving limited data sets, with little concern for performance.
% We are developing a new modern seismic data format based on ORNL's ADIOS
% libraries --called the Adaptable Seismic Data Format (ASDF)--  that is suitable
% for a variety of seismic workflows, allowing users to retain provenance related
% to observed and simulated seismograms. Here, we give examples from global
% adjoint tomography and exploration seismology, which are two of the extreme
% cases in seismic imaging. Pre-processing tools  (resampling, filtering, window
% selection, computing adjoint sources, etc.) are modified to take advantage of
% this new data format. We accommodate the ADIOS libraries in our numerical
% solvers to reduce the number of files that are read and written during
% simulations (i.e., meshes, kernels, models, etc.) to drastically decrease disk
% access. We adjust post-processing tools (i.e., summing, pre-conditioning and
% smoothing gradients, model updates, etc.) accordingly.  Moreover, parallel
% visualization tools, such as VisIt~\cite{Childs2011}, take advantage of metadata
% included in our ADIOS outputs to extract features and display massive data.
